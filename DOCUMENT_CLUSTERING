import pandas as pd
df = pd.read_csv("/content/data/CNN_news.csv")

df.info()

df.head()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

documents = df['news_text'].values.astype("U")

vectorizer = TfidfVectorizer(stop_words='english')
features = vectorizer.fit_transform(documents)

k = 5
model = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1)
model.fit(features)

df['cluster'] = model.labels_

df.head()

!pip install pyLDAvis
!pip install gensim

import gensim
from gensim.models import Phrases
#Prepare objects for LDA gensim implementation
from gensim import corpora
#Running LDA
from gensim import models

import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import re
%matplotlib inline
import string
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from IPython.display import HTML, display
import tabulate
import pandas as pd
import numpy as np
from PIL import Image
from wordcloud import WordCloud
import seaborn as sns
from gensim.models.coherencemodel import CoherenceModel
# from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords 
from nltk.stem import WordNetLemmatizer, PorterStemmer
# from nltk.stem.porter import *
import matplotlib.pyplot as plt

data = pd.read_csv("/content/data/CNN_news.csv")
data.head()

data['processed'] = ''
nltk.download('wordnet')
nltk.download('stopwords')

stop_words = stopwords.words('english')

stop_words.extend(['HTML'])
# initalizing the werdnet lemmatizer
lm = WordNetLemmatizer()

def processing(content):

   content =str(content).replace('\n', ' ').split(' ')
   #  removing these punctuations from tokens
   rx = re.compile('([&#.:?!-()])*')
   content = [rx.sub('', word) for word in content]
   #  removing stopwords
   content = [word.strip().lower() for word in content if word.strip().lower() not in stop_words]
   #  remove words whose length is greater than 1 and or alphabet only 
   content = [word for word in content if len(word)>1 and word.isalpha()]
   # lemmatizing the words to their basic form
   content = [lm.lemmatize(word) for word in content]
   return ' '.join(content)

for k in range(len(data)):
  data.iloc[k,-1] = processing(data.iloc[k,0])
# processed data
data.head()

b = data['processed'].tolist()
b = ' '.join(map(str, b))
# b = b.replace(',', ' ').lower().replace('-', '')

wordcloud = WordCloud(max_font_size=50, max_words=1200, background_color="white", random_state=42,
                      prefer_horizontal=0.60).generate(b.lower())
# max_font_size=40, max_words=1000, background_color="white",
                      # random_state=100, prefer_horizontal=0.50
plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
# plt.show()
plt.savefig('word_cloud.jpg')

from nltk.corpus import stopwords
sw = stopwords.words('english')
nltk.download("punkt")
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
ps = PorterStemmer()


#preprocessing
filtered_docs = []
for doc in data['processed']:    
  tokens = word_tokenize(doc)  
  # tmp = ""
  # for w in tokens:        
  #   if w not in sw:            
  #     tmp += ps.stem(w) + " "    
  filtered_docs.append(tokens)
print(filtered_docs)

lengths = [len(sentence.split(' ')) if len(sentence)>0 else 0 for sentence in data['processed']]
data['Lengths'] = lengths
plt.figure(figsize=(10,6))
data['Lengths'].hist(bins=30)
sns.despine(top=True, right=True, left=False, bottom=False)
plt.title('Number of words in each doc')
plt.xlabel('Words')
plt.ylabel('Length of doc')
plt.savefig('length_doc.jpg')
plt.show()

import gensim.corpora as corpora

#decomposing sentences into tokens
tokens = [sentence.split(' ') for sentence in data['processed'] ]

bigram = gensim.models.Phrases(tokens, min_count=2, threshold=100)
bigram_mod = gensim.models.phrases.Phraser(bigram)

# including bigrams as tokens 
sents = [ bigram_mod[token] for token in tokens]

# Create Dictionary to keep track of vocab
Dict = corpora.Dictionary(tokens)

print('Unique number of words after pre-processing, before filtering', len(Dict))
# filter the words that occurs in less than 3 documents and in more than 60% of documents
Dict.filter_extremes(no_below= 3, no_above=0.60 )
print('Unique words after filtering', len(Dict))

# Create Corpus
corpus = [Dict.doc2bow(sent) for sent in sents]

tfidf = gensim.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]

%%time
from gensim.models import CoherenceModel
import time
import os

scores = []
for k in range(3,11
              ):
    # LDA model
    lda_model = gensim.models.LdaModel(  corpus=corpus_tfidf, num_topics=k,
                                                 id2word=Dict, random_state=42)
    # to calculate score for coherence
    coherence_model_LDA = CoherenceModel(model=lda_model, texts=sents, dictionary=Dict, coherence='c_v')
    coherence_LDA = coherence_model_LDA.get_coherence()
    print("Topic",k,":", coherence_LDA)
    scores.append(coherence_LDA)

selected_topics = np.argmax(scores)+3

plt.figure(figsize=(14, 7))
plt.plot(list(range(3,11)), scores, marker='o', color='red')
sns.despine(top=True, right=True, left=False, bottom=False)

plt.locator_params(integer=True)
plt.title('Coherence score plotted against the number of topics for LDA')
plt.xlabel('Number of topics')
plt.ylabel('Coherence Scores')
plt.savefig('LDA_scores.jpg')
plt.show()

import pyLDAvis.gensim_models

LDA_model = gensim.models.LdaModel(corpus=corpus_tfidf, id2word=Dict, num_topics=selected_topics, 
                                           random_state=42, chunksize=128, passes=10 )

pyLDAvis.enable_notebook()
results = pyLDAvis.gensim_models.prepare(LDA_model, corpus_tfidf, Dict, sort_topics=False)
pyLDAvis.save_html(results, 'ldavis_english' +'.html')
results

# top words in each topic
LDA_model.print_topics()

# judged manually from pyldavis
topics_name = ['Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6', 'Topic 7', 'Topic 8', 'Topic 9']

# getting the most dominant topics from a trained model
predicted_topics = LDA_model[corpus_tfidf]

probs, topics = [], []
for k in predicted_topics:
  # sorting the probabilites
  k.sort(key=lambda x:x[1])
  # selecting the topic with greatest probability
  topics.append(topics_name[ k[0][0] ] )

data['Topics'] = topics
data.head(20)

# 1. Wordcloud of Top N words in each topic

from matplotlib import pyplot as plt
from wordcloud import WordCloud

cloud = WordCloud(background_color='white', width=2500, height=2000,
                  max_words=50, colormap='tab10', prefer_horizontal=1.0)
  
fig, axes = plt.subplots(1, 9, figsize=(17,8), sharex=True, sharey=True)

for i, ax in enumerate(axes.flatten()):
  
  fig.add_subplot(ax)
  if i>len(topics_name)-1:
    continue
  curr = data[data['Topics']==topics_name[i]]
  print(curr.shape)
  tokens = [tok for d in curr['processed'] for tok in d.split(' ')]
  cloud.generate(' '.join( tokens ))

  plt.gca().imshow(cloud)
  plt.gca().set_title( topics_name[i]+'\n')
  plt.gca().axis('off')

plt.axis('off')
plt.tight_layout()
plt.show()

def subcategory_plot(df, col):
    
    plt.figure(figsize=(20,8))
    bar_pub = list( df[col].value_counts().index[:8] )
    temp2 = df[df[col].isin(bar_pub)]

    sns.countplot(x=col, hue='Topics', data=temp2, palette="Accent_r")
    sns.despine()

    plt.savefig('bars_{}.png'.format(col))
    plt.show()

subcategory_plot(data, 'category')
